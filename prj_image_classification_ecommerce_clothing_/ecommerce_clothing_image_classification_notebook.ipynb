{"cells":[{"cell_type":"markdown","id":"aaa02648-9eae-45ba-893f-88440e8e4235","metadata":{},"source":["![clothing_classification](clothing_classification.png)\n"]},{"cell_type":"markdown","id":"ad5a988c-1095-485a-a88c-002400a872be","metadata":{},"source":["Fashion Forward is a new AI-based e-commerce clothing retailer.\n","They want to use image classification to automatically categorize new product listings, making it easier for customers to find what they're looking for. It will also assist in inventory management by quickly sorting items.\n","\n","As a data scientist tasked with implementing a garment classifier, your primary objective is to develop a machine learning model capable of accurately categorizing images of clothing items into distinct garment types such as shirts, trousers, shoes, etc."]},{"cell_type":"markdown","metadata":{},"source":["Automate product tagging for the e-commerce store using CNNs.\n","\n","Once trained (keeping the epochs to 1 or 2 to keep the run time down), store your predictions on the test set in a list named predictions.\n","\n","Calculate the accuracy, and per-class precision and recall for your classifier based on the predictions obtained. Store your metrics in variables named accuracy, precision, and recall. Use lists of the appropriate length for the precision and recall."]},{"cell_type":"markdown","metadata":{},"source":["# Package imports"]},{"cell_type":"code","execution_count":20,"id":"ea8065b7-84fc-4376-afef-6db731dec4b3","metadata":{"executionCancelledAt":null,"executionTime":3827,"lastExecutedAt":1713429494261,"lastExecutedByKernel":"613ad597-3acd-485b-b4bd-0a12e756578b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall"},"outputs":[],"source":["import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","from torchmetrics import Accuracy, Precision, Recall\n","from torchvision.datasets import ImageFolder\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","import torch\n","import os\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["#Define a transform to normalize the data\n","from torchvision import transforms\n","\n","# Train transforms (có augmentation)\n","train_transform = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(10),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","# Test transforms (không augmentation)\n","test_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n"]},{"cell_type":"markdown","metadata":{},"source":["# Reading and transforming the image data\n","- Training\n","- Validation"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["from torchvision import datasets\n","from torch.utils.data import DataLoader\n","\n","train_dataset = datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=train_transform)\n","test_dataset  = datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=test_transform)\n","\n","dataloader_train = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","dataloader_test = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"]}],"source":["classes = train_dataset.classes\n","print(classes)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'T-shirt/top': 0, 'Trouser': 1, 'Pullover': 2, 'Dress': 3, 'Coat': 4, 'Sandal': 5, 'Shirt': 6, 'Sneaker': 7, 'Bag': 8, 'Ankle boot': 9}\n"]}],"source":["print(train_dataset.class_to_idx)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["def imshow(sample_element, shape = (28, 28)):\n","    plt.imshow(sample_element[0].reshape(shape), cmap='gray')\n","    plt.title('Label = ' + str(sample_element[1]))\n","    plt.show()"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["imshow(train_data[0])"]},{"cell_type":"markdown","metadata":{},"source":["**1. Defining the CNN (Convolutional Neural Network) **\n","\n","Define a class containing all the appropriate layers, and a method to perform the forward pass of a batch of images.\n","\n","- Creating a class to contain the layers of a CNN\n","    * You could define a class that inherits from PyTorch's nn.module class.\n","\n","- Adding a convolutional layer\n","    * You could use PyTorch's nn.Conv2D() class to define the convolutional layer.\n","    * Create an instance of it in your CNN class's constructor and assign it to an instance variable such as self.conv.\n","\n","- Adding a Rectilinear Unit\n","    * You could use PyTorch's nn.ReLU() class.\n","    * Create an instance of it in your CNN class's constructor and assign it to an instance variable such as self.relu.\n","\n","- Adding a pooling layer\n","    * You could use PyTorch's nn.MaxPool2D() class.\n","    * Create an instance of it in your CNN class's constructor and assign it to an instance variable such as self.maxpool.\n","\n","- Adding a fully connected layer\n","    * You could use PyTorch's nn.Linear() class.\n","    * Create an instance of it in your CNN class's constructor and assign it to an instance variable such as self.fc.\n","    * You will also need to flatten the input first, which could be done with an instance of nn.Flatten\n","\n","- Defining a .forward() method\n","    * Finally, you'll need to define a .forward() method that passes the input through each layer and returns the output."]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["class CNN(nn.Module):\n","    def __init__(self, num_classes):\n","        super().__init__()\n","        # Define feature extractor\n","        self.feature_extractor = nn.Sequential(\n","            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2),\n","            nn.Flatten(),\n","        )\n","        # Define classifier\n","        self.classifier = nn.Linear(32*14*14, num_classes)\n","    \n","    def forward(self, x):  \n","        # Pass input through feature extractor and classifier\n","        x = self.feature_extractor(x)\n","        x = self.classifier(x)\n","        return x"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CNN(\n","  (feature_extractor): Sequential(\n","    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU()\n","    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (3): Flatten(start_dim=1, end_dim=-1)\n","  )\n","  (classifier): Linear(in_features=6272, out_features=10, bias=True)\n",")\n"]}],"source":["print(CNN(num_classes=10))"]},{"cell_type":"markdown","metadata":{},"source":["**2. Training the CNN**\n","Define a training loop that loops over the dataset, calculating the loss and propagating it backwards through the network.\n","\n","- Define a suitable loss criterion\n","    * PyTorch's nn.CrossEntropyLoss() could be used here, since this is a multi-class classification problem.\n","\n","- Define an optimizer\n","    * You could use PyTorch's optim.Adam() optimizer here.\n"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 0.5112\n","Epoch 2, Loss: 0.3833\n","Epoch 3, Loss: 0.3528\n"]}],"source":["# Define the model\n","model = CNN(num_classes=10)\n","# Define the loss function\n","criterion = nn.CrossEntropyLoss()\n","# Define the optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","for epoch in range(5):\n","    running_loss = 0.0\n","    # Iterate over training batches\n","    for images, labels in dataloader_train:\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","    \n","    epoch_loss = running_loss / len(dataloader_train)\n","    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")"]},{"cell_type":"markdown","metadata":{},"source":["**3. Testing the CNN**\n","Use your trained model to classify the images in the test set, and calculate the appropriate metrics.\n","\n","- Predict the category of each image in the test data.\n","    * You'll need to use the .forward() method on your CNN class to pass the test images through the network.\n","    * You could use torch.argmax() to find the category with the highest predicted probability.\n","\n","- Calculate the performance metrics\n","    * You could use Accuracy(), Precision(), and Recall() from torchmetrics to calculate the metrics."]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Precision: tensor([0.8704, 0.9789, 0.7898, 0.8680, 0.8167, 0.9807, 0.6801, 0.8871, 0.9717,\n","        0.9465])\n","Recall: tensor([0.7790, 0.9760, 0.8640, 0.8810, 0.8110, 0.9150, 0.6760, 0.9740, 0.9620,\n","        0.9380])\n"]}],"source":["from torchmetrics import Recall, Precision\n","#Define metrics\n","metric_precision = Precision(task=\"multiclass\", num_classes=10, average=None)\n","metric_recall = Recall(task='multiclass', num_classes=10, average=None)\n","\n","model.eval()\n","with torch.no_grad():\n","    for images, labels in dataloader_test:\n","        outputs = model(images)\n","        _, preds = torch.max(outputs, 1)\n","        metric_precision(preds, labels)\n","        metric_recall(preds, labels)\n","\n","precision = metric_precision.compute()\n","recall = metric_recall.compute()\n","print(f\"Precision: {precision}\")\n","print(f\"Recall: {recall}\")"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'T-shirt/top': 0.8703910708427429, 'Trouser': 0.9789367914199829, 'Pullover': 0.7897623181343079, 'Dress': 0.8679803013801575, 'Coat': 0.8167170286178589, 'Sandal': 0.9807074069976807, 'Shirt': 0.6800804734230042, 'Sneaker': 0.8870673775672913, 'Bag': 0.9717171788215637, 'Ankle boot': 0.9465186595916748}\n"]}],"source":["# Get precision per class\n","precision_per_class = {\n","    k: precision[v].item()\n","    for k, v \n","    in test_dataset.class_to_idx.items()\n","}\n","print(precision_per_class)"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'T-shirt/top': 0.7789999842643738, 'Trouser': 0.9760000109672546, 'Pullover': 0.8640000224113464, 'Dress': 0.8809999823570251, 'Coat': 0.8109999895095825, 'Sandal': 0.9150000214576721, 'Shirt': 0.6759999990463257, 'Sneaker': 0.9739999771118164, 'Bag': 0.9620000123977661, 'Ankle boot': 0.9380000233650208}\n"]}],"source":["#Get Recall per class\n","recall_per_class = {\n","    k: recall[v].item()\n","    for k, v\n","    in test_dataset.class_to_idx.items()\n","}\n","print(recall_per_class)"]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":5}
