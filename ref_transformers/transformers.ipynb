{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from torchmetrics import Accuracy\n",
    "# from sklearn.metrics import accuracy_score  # uncomment to use sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing out data: train-test split\n",
    "sentences = [\"I love this product\", \n",
    "             \"This is the worst service ever\", \n",
    "             \"I am very happy with my purchase\", \n",
    "             \"I will never buy this again\", \n",
    "             \"Absolutely fantastic experience\"]\n",
    "\n",
    "labels = [1, 0, 1, 0, 1]  # 1 for positive sentiment, 0 for negative sentiment\n",
    "\n",
    "#Train-test Split\n",
    "train_sentences = sentences[:4]\n",
    "train_labels = labels[:4]\n",
    "test_sentences = sentences[4:]\n",
    "test_labels = labels[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phuongnguyen/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Building the transformer model\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_size, heads, num_layers, dropout):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_size, nhead=heads),\n",
    "            num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_size, 2) # Output = 2 classes (positive/negative)\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "model = TransformerEncoder(embed_size=512, heads=8, num_layers=3, dropout=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'token_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/phuongnguyen/Documents/Code Github/deep-learning-pytorch-projects/prj_prediction_transformers/notebook.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/phuongnguyen/Documents/Code%20Github/deep-learning-pytorch-projects/prj_prediction_transformers/notebook.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentence, label \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(train_sentences, train_labels):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/phuongnguyen/Documents/Code%20Github/deep-learning-pytorch-projects/prj_prediction_transformers/notebook.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     tokens \u001b[39m=\u001b[39m sentence\u001b[39m.\u001b[39msplit()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/phuongnguyen/Documents/Code%20Github/deep-learning-pytorch-projects/prj_prediction_transformers/notebook.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([token_embeddings[tokens] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens], dim \u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/phuongnguyen/Documents/Code%20Github/deep-learning-pytorch-projects/prj_prediction_transformers/notebook.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     output \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/phuongnguyen/Documents/Code%20Github/deep-learning-pytorch-projects/prj_prediction_transformers/notebook.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(output, torch\u001b[39m.\u001b[39mtensor([label]))\n",
      "\u001b[1;32m/Users/phuongnguyen/Documents/Code Github/deep-learning-pytorch-projects/prj_prediction_transformers/notebook.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/phuongnguyen/Documents/Code%20Github/deep-learning-pytorch-projects/prj_prediction_transformers/notebook.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentence, label \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(train_sentences, train_labels):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/phuongnguyen/Documents/Code%20Github/deep-learning-pytorch-projects/prj_prediction_transformers/notebook.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     tokens \u001b[39m=\u001b[39m sentence\u001b[39m.\u001b[39msplit()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/phuongnguyen/Documents/Code%20Github/deep-learning-pytorch-projects/prj_prediction_transformers/notebook.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([token_embeddings[tokens] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens], dim \u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/phuongnguyen/Documents/Code%20Github/deep-learning-pytorch-projects/prj_prediction_transformers/notebook.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     output \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/phuongnguyen/Documents/Code%20Github/deep-learning-pytorch-projects/prj_prediction_transformers/notebook.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(output, torch\u001b[39m.\u001b[39mtensor([label]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'token_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "#Training the transformers\n",
    "for epoch in range(5):\n",
    "    for sentence, label in zip(train_sentences, train_labels):\n",
    "        tokens = sentence.split()\n",
    "        data = torch.stack([token_embeddings[tokens] for token in tokens], dim =1)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, torch.tensor([label]))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the transformers\n",
    "def predict(sentence):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tokens = sentence.split()\n",
    "        data = torch.stack([token_embeddings.get(token, torch.rand((1, 512)))\n",
    "                            for token in tokens], dim=1)\n",
    "        output = model(data)\n",
    "        predicted = torch.argmax(output, dim =1)\n",
    "        return \"Positive\" if predicted.item() == 1 else \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting on new text\n",
    "sample_sentence = \"This product can be better\"\n",
    "print(f\" '{sample_sentence}' is predicted as: {predict(sample_sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: {'happy': 0, 'i': 1, 'product': 2, 'will': 3, 'am': 4, 'fantastic': 5, 'ever': 6, 'this': 7, 'never': 8, 'with': 9, 'the': 10, 'buy': 11, 'service': 12, 'absolutely': 13, 'my': 14, 'love': 15, 'experience': 16, 'again': 17, 'is': 18, 'worst': 19, 'purchase': 20, 'very': 21}\n",
      "Epoch 1, Loss: 4.3669\n",
      "Epoch 2, Loss: 1.9446\n",
      "Epoch 3, Loss: 1.5617\n",
      "Epoch 4, Loss: 1.1911\n",
      "Epoch 5, Loss: 0.8063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phuongnguyen/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ========== Step 1: Tạo vocab ==========\n",
    "sentences = [\n",
    "    \"I love this product\",\n",
    "    \"This is the worst service ever\",\n",
    "    \"I am very happy with my purchase\",\n",
    "    \"I will never buy this again\",\n",
    "    \"Absolutely fantastic experience\"\n",
    "]\n",
    "\n",
    "labels = [1, 0, 1, 0, 1]\n",
    "\n",
    "train_sentences = sentences[:4]\n",
    "train_labels = labels[:4]\n",
    "test_sentences = sentences[4:]\n",
    "test_labels = labels[4:]\n",
    "\n",
    "# Simple tokenizer: lowercase + split\n",
    "tokenized = [s.lower().split() for s in sentences]\n",
    "vocab = {word: idx for idx, word in enumerate(set([w for s in tokenized for w in s]))}\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab:\", vocab)\n",
    "\n",
    "def encode(sentence):\n",
    "    return [vocab[w] for w in sentence.lower().split()]\n",
    "\n",
    "train_data = [encode(s) for s in train_sentences]\n",
    "test_data = [encode(s) for s in test_sentences]\n",
    "\n",
    "# ========== Step 2: Embedding + Transformer ==========\n",
    "embed_size = 32\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_size, heads, num_layers, dropout):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_size, nhead=heads, dropout=dropout),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_size, 2)  # binary classification\n",
    "\n",
    "    def forward(self, x):  \n",
    "        x = self.encoder(x)  # (seq_len, batch, embed)\n",
    "        x = x.mean(dim=0)    # mean pooling (batch, embed)\n",
    "        return self.fc(x)\n",
    "\n",
    "model = TransformerEncoder(embed_size=embed_size, heads=2, num_layers=1, dropout=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ========== Step 3: Training ==========\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for tokens, label in zip(train_data, train_labels):\n",
    "        token_ids = torch.tensor(tokens)           # (seq_len,)\n",
    "        embeds = embedding(token_ids)              # (seq_len, embed)\n",
    "        embeds = embeds.unsqueeze(1)               # (seq_len, batch=1, embed)\n",
    "\n",
    "        output = model(embeds)                     # (batch=1, num_classes=2)\n",
    "        loss = criterion(output, torch.tensor([label]))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
