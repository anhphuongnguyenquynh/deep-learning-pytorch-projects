{"cells":[{"cell_type":"markdown","id":"8b752817-8333-446e-9790-73d85b0aa14f","metadata":{},"source":["![Traffic](traffic.png)\n","\n","Traffic data fluctuates constantly or is affected by time. Predicting it can be challenging, but this task will help sharpen your time-series skills. With deep learning, you can use abstract patterns in data that can help boost predictability.\n","\n","Your task is to build a system that can be applied to help you predict traffic volume or the number of vehicles passing at a specific point and time. Determining this can help reduce road congestion, support new designs for roads or intersections, improve safety, and more! Or, you can use to help plan your commute to avoid traffic!\n","\n","The dataset provided contains the hourly traffic volume on an interstate highway in Minnesota, USA. It also includes weather features and holidays, which often impact traffic volume.\n","\n","Time to predict some traffic!\n","\n","### The data:\n","\n","The dataset is collected and maintained by UCI Machine Learning Repository. The target variable is `traffic_volume`. The dataset contains the following and has already been normalized and saved into training and test sets:\n","\n","`train_scaled.csv`, `test_scaled.csv`\n","| Column     | Type       | Description              |\n","|------------|------------|--------------------------|\n","|`temp`                   |Numeric            |Average temp in kelvin|\n","|`rain_1h`                |Numeric            |Amount in mm of rain that occurred in the hour|\n","|`snow_1h`                |Numeric            |Amount in mm of snow that occurred in the hour|\n","|`clouds_all`             |Numeric            |Percentage of cloud cover|\n","|`date_time`              |DateTime           |Hour of the data collected in local CST time|\n","|`holiday_` (11 columns)  |Categorical        |US National holidays plus regional holiday, Minnesota State Fair|\n","|`weather_main_` (11 columns)|Categorical     |Short textual description of the current weather|\n","|`weather_description_` (35 columns)|Categorical|Longer textual description of the current weather|\n","|`traffic_volume`         |Numeric            |Hourly I-94 ATR 301 reported westbound traffic volume|\n","|`hour_of_day`|Numeric|The hour of the day|\n","|`day_of_week`|Numeric|The day of the week (0=Monday, Sunday=6)|\n","|`day_of_month`|Numeric|The day of the month|\n","|`month`|Numeric|The number of the month|\n","|`traffic_volume`         |Numeric            |Hourly I-94 ATR 301 reported westbound traffic volume|"]},{"cell_type":"code","execution_count":2,"id":"d2e54daa-828a-420a-a204-f855de2ae375","metadata":{"executionCancelledAt":null,"executionTime":851,"lastExecutedAt":1729695181880,"lastExecutedByKernel":"f82efcd3-e7b9-41cb-8ce5-81dfbf2977fb","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import the relevant libraries\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader","outputsMetadata":{"0":{"height":223,"type":"dataFrame"},"1":{"height":222,"type":"dataFrame"}}},"outputs":[],"source":["# Import the relevant libraries\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader"]},{"cell_type":"code","execution_count":2,"id":"58e3a3b9-9367-43e7-b13b-1f111447478e","metadata":{"executionCancelledAt":null,"executionTime":167,"lastExecutedAt":1729694187994,"lastExecutedByKernel":"f82efcd3-e7b9-41cb-8ce5-81dfbf2977fb","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Read the traffic data from the CSV training and test files\ntrain_scaled_df = pd.read_csv('train_scaled.csv')\ntest_scaled_df = pd.read_csv('test_scaled.csv')\n\n# Convert the DataFrame to NumPy arrays\ntrain_scaled = train_scaled_df.to_numpy()\ntest_scaled = test_scaled_df.to_numpy()","outputsMetadata":{"0":{"height":223,"type":"dataFrame"}}},"outputs":[],"source":["# Read the traffic data from the CSV training and test files\n","train_scaled_df = pd.read_csv('train_scaled.csv')\n","test_scaled_df = pd.read_csv('test_scaled.csv')\n","\n","# Convert the DataFrame to NumPy arrays\n","train_scaled = train_scaled_df.to_numpy()\n","test_scaled = test_scaled_df.to_numpy()"]},{"cell_type":"code","execution_count":6,"id":"d29357d1-6cbb-4c86-ba49-ea31f6d8415e","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1729690564165,"lastExecutedByKernel":"f82efcd3-e7b9-41cb-8ce5-81dfbf2977fb","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Start coding here\n# Use as many cells as you like!"},"outputs":[],"source":["# Start coding here\n","# Use as many cells as you like!"]},{"cell_type":"markdown","metadata":{},"source":["Build a deep learning model that predicts traffic volume and helps tackle challenges like congestion, road design, and smarter commutes:\n","* Build a deep learning model using PyTorch to predict the traffic volume using the provided dataset. Initialize and save this model as traffic_model.\n","* Train and evaluate your model using an appropriate loss function. Save the final training loss as a tensor variable, final_training_loss (aim for less than 20).\n","* Predict the traffic volume against the test set and evaluate the performance using Mean Squared Error (MSE). Save your result as a tensor float, test_mse."]},{"cell_type":"markdown","metadata":{},"source":["**1. Prepare the data for modeling**\n","To model time-series data, you need to generate sequences of past values as inputs and predict the next value as the target. One way to do this is by writing a function and passing in the available data. These then need to be converted to PyTorch tensors and loaded.\n","- Creating sequences with a function\n","    - Define a function that takes the dataset, sequence length, and target column index as inputs.\n","    * Use a for loop to loop through the dataset: The loop should run from 0 to len(data) - sequence length. This ensures that you're able to get a full sequence of the desired length and a target value.\n","    * Extract the input sequence by slicing the dataset starting at index i and ending at i + sequence length.\n","    * Extract the target value that immediately follows the sequence (i.e., at index i + sequence length).\n","    * Append the input sequences to a list of inputs, and the targets to a list of outputs.\n","    * Return the inputs and targets as NumPy arrays at the end of the function using np.array().\n","    * Apply the function on both the training and test data set. You should end up with a X_train, y_train, X_test, y_test set, not necessarily with these names.\n","- Converting to tensors\n","    - Convert the input and output sequences from NumPy arrays to PyTorch tensors using torch.tensor().\n","    - PyTorch expects floating-point numbers for neural network inputs and targets, so you’ll need to convert the NumPy arrays accordingly. Do this by combining .astype(), np.float32 and .float().\n","    - Apply this to both your training and testing splits.\n","    - Use TensorDataset() to combine the feature and label tensors into a dataset.\n","- Loading data\n","    - Use DataLoader() to load the dataset in batches.\n","    * Batch size 64 is a common choice to balance training speed and memory usage.\n","    * shuffle can be set to True for the training set to reduce the risk of overfitting, but can be False for the test set."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["**2. Creating a neural network model**\n","Select and build an appropriate neural network that is good at handling time series data. Consider using Recurrent Neural Networks, which are designed to capture temporal dependencies in sequential data.\n","- Choosing the right neural network\n","    * RNNs are ideal for handling time-series data because they can retain information from previous time steps.\n","    * Use nn.LSTM() or nn.GRU() to capture both short and long term dependencies.\n","- Defining the __init__() method and RNN Layer\n","    * In the __init__() method, define the layers of your neural network, starting with the RNN layer, for example, use nn.LSTM() to define the LSTM layer.\n","    * The LSTM layer could include the input_size, hidden_size, num_layers, and batch_first.\n","    * input_size should match the number of input features you have for each time step. If using the full dataset, that's 66 features.\n","    * hidden_size of 64 should provide a good balance for speed and capturing complex patterns.\n","    * Try more than more layer in num_layers to have the model learn more complex patterns.\n","    * batch_first=True ensures the input tensor has the shape (batch_size, sequence_length, input_size).\n","    * Add any additional layers after this, but it's recommended to include an activation function and a fully connected later to map the output to the target value.\n","    * The fully connected layer can be added with nn.Linear(), defining the size of the input to and the output from the layer. The prediction here is a single value (traffic volume), so the output size is 1.\n","- Selecting an activation function\n","    - Include an activation function to the model learn complex relationships between inputs and outputs.\n","    - Try experimenting with different activation functions and see how the model trains. For example: nn.LeakyReLU(), nn.ReLU(), or nn.Sigmoid().\n","- Writing the forward method\n","    * The forward() method defines how data flows through the network during training and inference.\n","    * Pass your date through the RNN layer, such as the LSTM() layer. This returns two values: the hidden states from all time steps, and the hidden and cell states from the last time step for each layer of the LSTM. You only want to extract the final hidden state (h_0) to predict traffic volume.\n","    * Pass the final hidden state through the fully connected layer.\n","    * Apply the activation function.\n","    * Return the final output, or prediction."]},{"cell_type":"markdown","metadata":{},"source":["**3. Training the model**\n","Set up and train the neural network model to predict traffic volume. This involves initializing the model, selecting an appropriate loss function and optimizer, and running the training loop for multiple epochs to minimize the loss function.\n","- Initializing the model\n","    * Create an instance of your model class to initialize the neural network, saving it to traffic_model.\n","    * For example, if your model is called Net(), this would look like traffic_model = Net().\n","- Choosing the loss function\n","    - Choose a loss function that is appropriate for regression tasks. One such loss function is Mean Squared Error (MSE) (nn.MSELoss()).\n","- Selecting an optimizer\n","    * Use an optimizer to update the model's weights during training.\n","    * Try experimenting with different optimizers such as optim.Adam(), optim.SGD() or optim.Adagrad().\n","    * Set the learning rate appropriately. Increase the learning rate if you don't seem to be reaching convergence, or decrease it if the training loss is fluctuating a lot. Smaller learning rates lead to more gradual updates, which can help with convergence but may take longer.\n","- Running a training loop\n","    - Run the training loop for 2 epochs to submit the project, you can increase this after to see how it impacts your model training.\n","    * For each epoch, iterate over the batches of training data with a for loop.\n","    * Zero the gradients with optimizer.zero_grad().\n","    * Pass the batch inputs through the model to generate predictions.\n","    * Compare the predictions to the actual labels using your chosen loss function.\n","    * Use loss.backward() to calculate the gradients.\n","    * Use optimizer.step()` to adjust the model's weights based on the gradients.\n","    * Print the loss for each epoch for monitoring. After the final epoch, store the final training loss in a variable (final_training_loss)."]},{"cell_type":"markdown","metadata":{},"source":["**4. Evaluating the model**\n","\n","After training the model, it's essential to evaluate its performance on unseen data (test set). This step involves running the model in evaluation mode, collecting the predictions, and comparing them to the actual labels using an appropriate metric.\n","- Setting evaluation mode\n","    * Set the model to evaluation mode by using .eval().\n","- Running an evaluation loop\n","    * Start by disabling gradient calculation. This is not needed during evaluation. You can do this with torch.no_grad().\n","    * Loop over the test data using the test DataLoader. For each batch of sequences, pass them through the model to get predictions.\n","    * Squeeze the output with .squeeze() to ensure the shape is appropriate.\n","    * Store the predictions and actual labels in lists for later evaluation.\n","    * After the loop, use torch.cat() to concatenate all predictions and labels into two tensors, making it easier to compute the final evaluation metric over the entire test set.\n","- Calculating the MSE\n","    - Evaluate how well the model's predictions match the actual traffic volumes with Mean Squared Error (MSE), F.mse_loss().\n","    * Save this to test_mse."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Create sequences\n","\n","import numpy as np \n","def create_sequences(data, seq_length):\n","    xs, ys = [], []\n","    for i in range(len(data) - seq_length):\n","        x = data.iloc[i:(i+seq_length), 1]\n","        y = data.iloc[i+seq_length, 1]\n","        xs.append(x)\n","        ys.append(y)\n","    return np.array(xs), np.array(ys)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Tensor Dataset\n","X_train, y_train = create_sequences(train_data, seq_length)\n","print(X_train.shape, y_train.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Convert to Torch Dataset\n","from torch.utils.data import TensorDataset \n","\n","dataset_train = TensorDataset(\n","    torch.from_numpy(X_train).float(),\n","    torch.from_numpy(y_train).float()\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#RNN model\n","class RNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.rnn = nn.RNN(\n","            input_size=1, \n","            hidden_size=32,\n","            num_layers=2,\n","            batch_first=True,\n","        )\n","        self.fc = nn.Linear(32, 1)\n","    \n","    def forward(self, x):\n","        h0 = torch.zeros(2, x.size(0), 32)\n","        out, _ = self.rnn(x, h0)\n","        out = self.fc((out[:, -1, :]))\n","        return out "]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["#LSTM Model\n","class LSTM(nn.Module):\n","    def __init__(self, input_size):\n","        super().__init__()\n","        self.lstm = nn.LSTM(\n","            input_size=1,\n","            hidden_size=32,\n","            num_layers=2,\n","            batch_first=True, \n","        )\n","        self.fc = nn.Linear(32, 1)\n","    \n","    def forward(self, x):\n","        h0 = torch.zeros(2, x.size(0), 32)\n","        c0 = torch.zeros(2, x.size(0), 32)\n","        out, _ = self.lstm(x, (h0, c0))\n","        out = self.fc(out[:, -1, :])\n","        return out "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#GRU model\n","class GRU(nn.Module):\n","    def __init__(self, input_size):\n","        super().__init__()\n","        self.gru = nn.GRU(\n","            input_size=1,\n","            hidden_size=32,\n","            num_layers=2,\n","            batch_first=True, \n","        )\n","        self.fc = nn.Linear(32, 1)\n","    \n","    def forward(self, x):\n","        h0 = torch.zeros(2, x.size(0), 32)\n","        out, _ = self.gru(x, h0)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["criterion = nn.MSELoss()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Training loop\n","model = LSTM(input_size=1)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(\n","    model.parameters(), lr=0.001\n",")\n","\n","for epoch in range(num_epochs):\n","    for seqs, labels in dataloader_train:\n","        seqs = seqs.view(32, 96, 1)\n","        outputs = model(seqs)\n","        loss = criterion(outputs, labels)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Evaluation loop\n","\n","mse = torchmetrics.MeanSquaredError()\n","\n","model.eval()\n","with torch.no_grad():\n","    for seqs, labels in test_loader:\n","        seqs = seqs.view(32, 96, 1)\n","        outputs = model(seqs).squeeze()\n","        mse(outputs, labels)\n","\n","print(f\"Test MSE: {mse.compute()}\")"]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":5}
